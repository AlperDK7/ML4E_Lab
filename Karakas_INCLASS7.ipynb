{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my key: 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N11Ee3GJmywu",
    "outputId": "64e10b54-dad3-48e2-c266-617670a17e82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: /usr/share/pip-wheels\n",
      "Collecting openai\n",
      "  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/26/a1/75474477af2a1dae3a25f80b72bbaf20e8296191ece7fff2f67984206f33/openai-1.12.0-py3-none-any.whl.metadata\n",
      "  Downloading openai-1.12.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from openai) (3.5.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Obtaining dependency information for distro<2,>=1.7.0 from https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl.metadata\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Obtaining dependency information for httpx<1,>=0.23.0 from https://files.pythonhosted.org/packages/41/7b/ddacf6dcebb42466abd03f368782142baa82e08fc0c1f8eaa05b4bae87d5/httpx-0.27.0-py3-none-any.whl.metadata\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Obtaining dependency information for pydantic<3,>=1.9.0 from https://files.pythonhosted.org/packages/db/dc/afecbd9650f486889181c6d1a0d675b580c06253ea7e304588e4c7485bdb/pydantic-2.6.1-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.6.1-py3-none-any.whl.metadata (83 kB)\n",
      "\u001b[2K     \u001b[38;5;70m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from openai) (4.7.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from wikipedia) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/2c/93/13f25f2f78646bab97aee7680821e30bd85b2ff0fc45d5fdf5393b79716d/httpcore-1.0.4-py3-none-any.whl.metadata\n",
      "  Downloading httpcore-1.0.4-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Obtaining dependency information for h11<0.15,>=0.13 from https://files.pythonhosted.org/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl.metadata\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Obtaining dependency information for annotated-types>=0.4.0 from https://files.pythonhosted.org/packages/28/78/d31230046e58c207284c6b2c4e8d96e6d3cb4e52354721b944d3e1ee4aa5/annotated_types-0.6.0-py3-none-any.whl.metadata\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Obtaining dependency information for pydantic-core==2.16.2 from https://files.pythonhosted.org/packages/ad/03/1cac52dfe893109a1571956755061df771457f33cb55264baed8f89635d6/pydantic_core-2.16.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pydantic_core-2.16.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.16)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from beautifulsoup4->wikipedia) (2.4)\n",
      "Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[38;5;70m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[38;5;70m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[38;5;70m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.6.1-py3-none-any.whl (394 kB)\n",
      "\u001b[2K   \u001b[38;5;70m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.8/394.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.16.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[38;5;70m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[38;5;70m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=c2ec4da1ea8c23bd8d5b2200b6522e0e499fa34447c4beb51a7e48866162e05a\n",
      "  Stored in directory: /home/36cd4c29-5390-4fac-8c93-edfb2980f04f/.cache/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: pydantic-core, h11, distro, annotated-types, wikipedia, pydantic, httpcore, httpx, openai\n",
      "\u001b[33m  WARNING: The script distro is installed in '/home/36cd4c29-5390-4fac-8c93-edfb2980f04f/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script httpx is installed in '/home/36cd4c29-5390-4fac-8c93-edfb2980f04f/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script openai is installed in '/home/36cd4c29-5390-4fac-8c93-edfb2980f04f/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.6.0 distro-1.9.0 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.12.0 pydantic-2.6.1 pydantic-core-2.16.2 wikipedia-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "Q2A8TGhKm3i5"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7E9HEMJSX-3T"
   },
   "source": [
    "# 1.) Set up OpenAI and the enviornment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zwwdkZDYDZN"
   },
   "outputs": [],
   "source": [
    "# DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "8IiKS0snlpYP"
   },
   "outputs": [],
   "source": [
    "apikey = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key = openai.api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOXc5_BTm9HP"
   },
   "source": [
    "# 2.) Use the wikipedia api to get a function that pulls in the text of a wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "-v7OYamHlrEB"
   },
   "outputs": [],
   "source": [
    "page_titles = [\"Aritificial Intelligence\", \"UCLA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_title = page_titles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = wikipedia.search(page_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = wikipedia.page(search_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "TgY2FkTdmhTH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['API_URL',\n",
       " 'BeautifulSoup',\n",
       " 'Decimal',\n",
       " 'DisambiguationError',\n",
       " 'HTTPTimeoutError',\n",
       " 'ODD_ERROR_MESSAGE',\n",
       " 'PageError',\n",
       " 'RATE_LIMIT',\n",
       " 'RATE_LIMIT_LAST_CALL',\n",
       " 'RATE_LIMIT_MIN_WAIT',\n",
       " 'RedirectError',\n",
       " 'USER_AGENT',\n",
       " 'WikipediaException',\n",
       " 'WikipediaPage',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'cache',\n",
       " 'datetime',\n",
       " 'debug',\n",
       " 'donate',\n",
       " 'exceptions',\n",
       " 'geosearch',\n",
       " 'languages',\n",
       " 'page',\n",
       " 'random',\n",
       " 're',\n",
       " 'requests',\n",
       " 'search',\n",
       " 'set_lang',\n",
       " 'set_rate_limiting',\n",
       " 'set_user_agent',\n",
       " 'stdout_encode',\n",
       " 'suggest',\n",
       " 'summary',\n",
       " 'sys',\n",
       " 'time',\n",
       " 'timedelta',\n",
       " 'unicode_literals',\n",
       " 'util',\n",
       " 'wikipedia']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "Kw5H5jMlmmS3"
   },
   "outputs": [],
   "source": [
    "#page.content\n",
    "def get_wikipedia_content(page_title):\n",
    "    search_results = wikipedia.search(page_title)\n",
    "    page = wikipedia.page(search_results[0])\n",
    "    return(page.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "ZF3BiZyXltYO"
   },
   "outputs": [],
   "source": [
    "content = get_wikipedia_content(page_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9aruncMmubX"
   },
   "source": [
    "# 3.) Build a chatgpt bot that will analyze the text given and try to locate any false info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "Bmai3B6Dmw3O"
   },
   "outputs": [],
   "source": [
    "chat_completions = client.chat.completions.create(\n",
    "    model = \"gpt-4\",\n",
    "    messages = [\n",
    "        {\"role\" : \"system\", \"content\": \"I will be giving you an article. I am looking for false information. Please concisely list only the false information found. I want to capture all potentially false information, even if the potential is small.\"},\n",
    "        {\"role\" : \"user\", \"content\": content[:8180]}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "1jI-un5PnDjg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='1. UCLA is not the second-oldest of the ten-campus University of California system. It is the third oldest after UC Berkeley and UC Davis.\\n2. UCLA has not been represented in every Olympics since its founding. The first year UCLA athletes participated in the Olympics was 1920, not at the 1912 or 1916 Games.\\n3. The number of Bruins who have made Olympic teams is 233, not 410. \\n4. The undergraduate applications for Fall 2022 is not 174,914, the admissions data for this year has yet to be released. In 2021, UCLA received more than 139,500 applications.\\n5. Blanket statement \"San José State University which later evolved into UCLA\" is misleading. The Los Angeles branch of California State Normal School, which later transitioned and became UCLA, and San José State University are separate entities.\\n6. The notion of UCLA being \"treated as an off-site department of the main campus in Berkeley\" might be misleading. While it\\'s true that UC Berkeley was the original campus and UCLA started as a branch, UCLA always operated as an independent institution with its own curriculum and campus.\\n7. UCLA has won 118 NCAA team championships, not 121. Stanford University leads with 126 NCAA team titles. \\n8. UCLA Lab School was not originally the Los Angeles branch of the California State Normal School. The Branch State Normal School of California, which would eventually become UCLA, was separately established and housed its own training school for teachers.  \\n9. The Humanities Building is not now the Renee and David Kaplan Hall. The Humanities Building is still known as the Humanities Building. \\n10. UCLA did not partner with the Tongva in 2022 for the caretaking and landscaping of various areas of the campus; no such agreement can be found on record.', role='assistant', function_call=None, tool_calls=None))]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completions.choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "_TMKFGN4nDJ4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. UCLA is not the second-oldest of the ten-campus University of California system. It is the third oldest after UC Berkeley and UC Davis.\n",
      "2. UCLA has not been represented in every Olympics since its founding. The first year UCLA athletes participated in the Olympics was 1920, not at the 1912 or 1916 Games.\n",
      "3. The number of Bruins who have made Olympic teams is 233, not 410. \n",
      "4. The undergraduate applications for Fall 2022 is not 174,914, the admissions data for this year has yet to be released. In 2021, UCLA received more than 139,500 applications.\n",
      "5. Blanket statement \"San José State University which later evolved into UCLA\" is misleading. The Los Angeles branch of California State Normal School, which later transitioned and became UCLA, and San José State University are separate entities.\n",
      "6. The notion of UCLA being \"treated as an off-site department of the main campus in Berkeley\" might be misleading. While it's true that UC Berkeley was the original campus and UCLA started as a branch, UCLA always operated as an independent institution with its own curriculum and campus.\n",
      "7. UCLA has won 118 NCAA team championships, not 121. Stanford University leads with 126 NCAA team titles. \n",
      "8. UCLA Lab School was not originally the Los Angeles branch of the California State Normal School. The Branch State Normal School of California, which would eventually become UCLA, was separately established and housed its own training school for teachers.  \n",
      "9. The Humanities Building is not now the Renee and David Kaplan Hall. The Humanities Building is still known as the Humanities Building. \n",
      "10. UCLA did not partner with the Tongva in 2022 for the caretaking and landscaping of various areas of the campus; no such agreement can be found on record.\n"
     ]
    }
   ],
   "source": [
    "print(chat_completions.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "6FKAJVXSoayA"
   },
   "outputs": [],
   "source": [
    "def chatgpt_error_correction(text):\n",
    "    chat_completions = client.chat.completions.create(\n",
    "    model = \"gpt-4\",\n",
    "    messages = [\n",
    "        {\"role\" : \"system\", \"content\": \"I will be giving you an article. I am looking for false information. Please concisely list only the false information found. I want to capture all potentially false information, even if the potential is small.\"},\n",
    "        {\"role\" : \"user\", \"content\": text[:8180]}]\n",
    "    )\n",
    "    print(chat_completions.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPw5LyPEobmk"
   },
   "source": [
    "# 4.) Make a for loop and check a few wikipedia pages and return a report of any potentially false info via wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "V7cuhML2ocGn"
   },
   "outputs": [],
   "source": [
    "page_titles = [\"Aritificial Intelligence\", \"UCLA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________Aritificial Intelligence\n",
      "1. The claim that the term \"back-propagating errors\" was introduced in 1962 by Frank Rosenblatt is false. The concept and term of backpropagation in the context of neural networks was introduced by Rumelhart, Hinton, and Williams in 1986.\n",
      "\n",
      "2. The article incorrectly states that the first implementation of the perceptron, the first artificial neural network, was funded by the United States Office of Naval Research in 1958. However, the perceptron model was proposed by psychologist Frank Rosenblatt, but the article does not provide correct details on its first implementation.\n",
      "\n",
      "3. The statement that digital computers evolved from the von Neumann model is misleading. The von Neumann architecture is a design model for a stored-program digital computer, but not all digital computers necessarily follow this model.\n",
      "\n",
      "4. The article falsely claims that the first deep learning Multi Layer Perceptron (MLP) was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965. While Ivakhnenko and Lapa made significant contributions to the development of MLPs, the concept of deep learning wasn’t recognised until much later.\n",
      "\n",
      "5. The term \"AI Winter\" is attributed to a decrease in government funding and an increased stress on symbolic artificial intelligence in the United States and other Western countries, which is too limited a definition. The term indicates a period of disappointment in the progress of AI technologies, resulting in decreased funding and interest, not solely due to government funding or symbolic AI.\n",
      "\n",
      "6. The article incorrectly states that in 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. In fact, the ReLU activation function became widely used much later.\n",
      "\n",
      "7. The claim that The Group Method of Data Handling was the first deep learning MLP is incorrect. The Group Method of Data Handling is a method used in the modeling of complex systems, not a deep learning MLP.\n",
      "___________UCLA\n",
      "1. UCLA's academic roots began as the Southern Branch of the California State Normal School in 1881. This is incorrect because it was established as the southern branch in 1919, not 1881.\n",
      "\n",
      "2. The university is the second-oldest of the ten-campus University of California system after the University of California, Berkeley. This is not true; it is not the second-oldest. \n",
      "\n",
      "3. UCLA offers 337 undergraduate and graduate degree programs. The actual number of programs UCLA offers is 125 undergraduate degrees and over 200 options for graduate degrees, for a total of over 325, not 337.\n",
      "\n",
      "4. It received 174,914 undergraduate applications for Fall 2022. As of now, we still do not have the exact total number of applications for fall 2022.\n",
      "\n",
      "5. Three of UCLA's professional schools - Medicine, Dentistry, and Public Health - only offer graduate-level professional health science degrees. This is incorrect. Although these schools primarily focus on graduate and professional education, several of them also offer undergraduate programs.\n",
      "\n",
      "6. The university has won 121 NCAA team championships. This is incorrect. As of 2021, UCLA has won 119 NCAA team championships.\n",
      "\n",
      "7. The university has had 270 Olympic medalists, 136 gold, 71 silver and 63 bronze. This is incorrect. As of 2021, UCLA-affiliated athletes have won 261 Olympic medals: 133 gold, 66 silver, and 62 bronze.\n",
      "\n",
      "8. 55 associated faculty members have been elected to the National Academy of Sciences, 29 to the National Academy of Engineering, and 41 to the National Academy of Medicine. This is false. The actual numbers are different. As of 2021, UCLA has 47 faculty members in the National Academy of Sciences, 30 in the National Academy of Engineering, and 37 in the National Academy of Medicine.\n",
      "\n",
      "9. The nickname \"Bruins\" was offered by the University of California, Berkeley student council. This isn't true. The nickname 'Bruins' was decided by a student contest.\n",
      "\n",
      "10. The first undergraduate classes on the new campus were held in 1929 with 5,500 students. This is incorrect. The enrollment in 1929 was 4,000 not 5,500.\n",
      "\n",
      "11. UCLA was permitted to award the master's degree in 1933, and the doctorate in 1936. This is not true; UCLA was authorized to confer the master's degree in 1933 but didn't get its authorization to award doctorates until 1936.\n"
     ]
    }
   ],
   "source": [
    "for page_title in page_titles:\n",
    "    try:\n",
    "        print(\"___________\" + page_title)\n",
    "        content = get_wikipedia_content(page_title)\n",
    "        chatgpt_error_correction(content)\n",
    "    except:\n",
    "        print(\"ERROR\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
